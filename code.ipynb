{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.layers import LSTM,Dense,Input,Dropout\n",
    "from tensorflow.keras.models import Sequential,Model,load_model \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading and Parsing the Midi File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.93it/s]\n",
      "C:\\Users\\nandk\\AppData\\Local\\Temp\\ipykernel_21468\\1780198071.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  notes_array = np.array([read_files(i) for i in tqdm(all_files,position=0,leave=True)])\n"
     ]
    }
   ],
   "source": [
    "def read_files(file):\n",
    "  notes=[]\n",
    "  notes_to_parse=None\n",
    "  #parse the midi file\n",
    "  midi=converter.parse(file)\n",
    "  #seperate all instruments from the file\n",
    "  instrmt=instrument.partitionByInstrument(midi)\n",
    "\n",
    "  for part in instrmt.parts:\n",
    "    #fetch data only of Piano instrument\n",
    "    if 'Piano' in str(part):\n",
    "      notes_to_parse=part.recurse()\n",
    "\n",
    "      #iterate over all the parts of sub stream elements\n",
    "      #check if element's type is Note or chord\n",
    "      #if it is chord split them into notes\n",
    "      for element in notes_to_parse:\n",
    "        if type(element)==note.Note:\n",
    "          notes.append(str(element.pitch))\n",
    "        elif type(element)==chord.Chord:\n",
    "          notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "  #return the list of notes\n",
    "  return notes\n",
    "\n",
    "#retrieve paths recursively from inside the directories/files\n",
    "file_path=[\"borodin\" , \"bach\" , \"brahms\"]\n",
    "all_files=glob.glob('All Midi Files/'+file_path[0]+'/*.mid',recursive=True)\n",
    "\n",
    "#reading each midi file\n",
    "notes_array = np.array([read_files(i) for i in tqdm(all_files,position=0,leave=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Notes: 165\n",
      "\n",
      "Frequency notes\n",
      "30 : 48\n",
      "50 : 35\n",
      "70 : 24\n",
      "90 : 17\n"
     ]
    }
   ],
   "source": [
    "#unique notes\n",
    "notess = sum(notes_array,[]) \n",
    "unique_notes = list(set(notess))\n",
    "print(\"Unique Notes:\",len(unique_notes))\n",
    "\n",
    "#notes with their frequency\n",
    "freq=dict(map(lambda x: (x,notess.count(x)),unique_notes))\n",
    "\n",
    "#get the threshold frequency\n",
    "print(\"\\nFrequency notes\")\n",
    "for i in range(30,100,20):\n",
    "  print(i,\":\",len(list(filter(lambda x:x[1]>=i,freq.items()))))\n",
    "\n",
    "#filter notes greater than threshold i.e. 30\n",
    "freq_notes=dict(filter(lambda x:x[1]>=30,freq.items()))\n",
    "\n",
    "#create new notes using the frequent notes\n",
    "new_notes=[[i for i in j if i in freq_notes] for j in notes_array]\n",
    "\n",
    "#dictionary having key as note index and value as note\n",
    "ind2note=dict(enumerate(freq_notes))\n",
    "\n",
    "#dictionary having key as note and value as note index\n",
    "note2ind=dict(map(reversed,ind2note.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and Output Sequence for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestep\n",
    "timesteps=60\n",
    "\n",
    "#store values of input and output\n",
    "x=[] ; y=[]\n",
    "\n",
    "for i in new_notes:\n",
    "  for j in range(0,len(i)-timesteps):\n",
    "    #input will be the current index + timestep\n",
    "    #output will be the next index after timestep\n",
    "    inp=i[j:j+timesteps] ; out=i[j+timesteps]\n",
    "\n",
    "    #append the index value of respective notes \n",
    "    x.append(list(map(lambda x:note2ind[x],inp)))\n",
    "    y.append(note2ind[out])\n",
    "\n",
    "x_new=np.array(x) \n",
    "y_new=np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape input and output for the model\n",
    "x_new = np.reshape(x_new,(len(x_new),timesteps,1))\n",
    "y_new = np.reshape(y_new,(-1,1))\n",
    "\n",
    "#split the input and value into training and testing sets\n",
    "#80% for training and 20% for testing sets\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_new,y_new,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 60, 300)           362400    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 60, 300)           0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 300)               721200    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 300)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 300)               90300     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 48)                14448     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,188,348\n",
      "Trainable params: 1,188,348\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create the model\n",
    "model = Sequential()\n",
    "#create two stacked LSTM layer with the latent dimension of 256\n",
    "model.add(LSTM(300,return_sequences=True,input_shape=(x_new.shape[1],x_new.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(300,activation='relu'))\n",
    "\n",
    "#fully connected layer for the output with softmax activation\n",
    "model.add(Dense(len(note2ind),activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "18/18 [==============================] - 18s 836ms/step - loss: 3.8156 - accuracy: 0.0391 - val_loss: 3.7689 - val_accuracy: 0.0380\n",
      "Epoch 2/80\n",
      "18/18 [==============================] - 17s 952ms/step - loss: 3.7555 - accuracy: 0.0464 - val_loss: 3.7089 - val_accuracy: 0.0714\n",
      "Epoch 3/80\n",
      "18/18 [==============================] - 15s 816ms/step - loss: 3.6967 - accuracy: 0.0593 - val_loss: 3.6966 - val_accuracy: 0.0669\n",
      "Epoch 4/80\n",
      "18/18 [==============================] - 16s 923ms/step - loss: 3.6695 - accuracy: 0.0691 - val_loss: 3.6657 - val_accuracy: 0.0623\n",
      "Epoch 5/80\n",
      "18/18 [==============================] - 15s 854ms/step - loss: 3.6373 - accuracy: 0.0783 - val_loss: 3.6334 - val_accuracy: 0.0866\n",
      "Epoch 6/80\n",
      "18/18 [==============================] - 17s 931ms/step - loss: 3.5996 - accuracy: 0.0722 - val_loss: 3.6074 - val_accuracy: 0.0790\n",
      "Epoch 7/80\n",
      "18/18 [==============================] - 20s 1s/step - loss: 3.5422 - accuracy: 0.0912 - val_loss: 3.5605 - val_accuracy: 0.1049\n",
      "Epoch 8/80\n",
      "18/18 [==============================] - 16s 894ms/step - loss: 3.4827 - accuracy: 0.0954 - val_loss: 3.4560 - val_accuracy: 0.1018\n",
      "Epoch 9/80\n",
      "18/18 [==============================] - 17s 935ms/step - loss: 3.4548 - accuracy: 0.1079 - val_loss: 3.4314 - val_accuracy: 0.1125\n",
      "Epoch 10/80\n",
      "18/18 [==============================] - 25s 1s/step - loss: 3.3438 - accuracy: 0.1208 - val_loss: 3.3794 - val_accuracy: 0.1277\n",
      "Epoch 11/80\n",
      "18/18 [==============================] - 19s 1s/step - loss: 3.2593 - accuracy: 0.1261 - val_loss: 3.3124 - val_accuracy: 0.1353\n",
      "Epoch 12/80\n",
      "18/18 [==============================] - 18s 983ms/step - loss: 3.1732 - accuracy: 0.1372 - val_loss: 3.3351 - val_accuracy: 0.1246\n",
      "Epoch 13/80\n",
      "18/18 [==============================] - 18s 1s/step - loss: 3.1293 - accuracy: 0.1364 - val_loss: 3.2715 - val_accuracy: 0.1292\n",
      "Epoch 14/80\n",
      "18/18 [==============================] - 18s 1s/step - loss: 3.0183 - accuracy: 0.1638 - val_loss: 3.2693 - val_accuracy: 0.1383\n",
      "Epoch 15/80\n",
      "18/18 [==============================] - 17s 959ms/step - loss: 2.9057 - accuracy: 0.1843 - val_loss: 3.2049 - val_accuracy: 0.1611\n",
      "Epoch 16/80\n",
      "18/18 [==============================] - 19s 1s/step - loss: 2.8025 - accuracy: 0.2029 - val_loss: 3.1398 - val_accuracy: 0.1733\n",
      "Epoch 17/80\n",
      "18/18 [==============================] - 18s 1s/step - loss: 2.7027 - accuracy: 0.2116 - val_loss: 3.2069 - val_accuracy: 0.1505\n",
      "Epoch 18/80\n",
      "18/18 [==============================] - 18s 995ms/step - loss: 2.6321 - accuracy: 0.2413 - val_loss: 3.1068 - val_accuracy: 0.1641\n",
      "Epoch 19/80\n",
      "18/18 [==============================] - 18s 976ms/step - loss: 2.4890 - accuracy: 0.2690 - val_loss: 3.0697 - val_accuracy: 0.1748\n",
      "Epoch 20/80\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.3510 - accuracy: 0.2975 - val_loss: 3.0418 - val_accuracy: 0.1672\n",
      "Epoch 21/80\n",
      "18/18 [==============================] - 17s 943ms/step - loss: 2.2546 - accuracy: 0.3116 - val_loss: 3.0270 - val_accuracy: 0.1824\n",
      "Epoch 22/80\n",
      "18/18 [==============================] - 18s 988ms/step - loss: 2.1971 - accuracy: 0.3229 - val_loss: 3.0338 - val_accuracy: 0.1641\n",
      "Epoch 23/80\n",
      "18/18 [==============================] - 20s 1s/step - loss: 2.0629 - accuracy: 0.3579 - val_loss: 2.9799 - val_accuracy: 0.2128\n",
      "Epoch 24/80\n",
      "18/18 [==============================] - 20s 1s/step - loss: 1.9615 - accuracy: 0.3913 - val_loss: 2.9883 - val_accuracy: 0.2082\n",
      "Epoch 25/80\n",
      "18/18 [==============================] - 17s 957ms/step - loss: 1.8963 - accuracy: 0.3948 - val_loss: 3.0059 - val_accuracy: 0.2006\n",
      "Epoch 26/80\n",
      "18/18 [==============================] - 15s 824ms/step - loss: 1.8053 - accuracy: 0.4179 - val_loss: 3.0327 - val_accuracy: 0.1976\n",
      "Epoch 27/80\n",
      "18/18 [==============================] - 16s 875ms/step - loss: 1.7146 - accuracy: 0.4491 - val_loss: 2.9949 - val_accuracy: 0.2082\n",
      "Epoch 28/80\n",
      "18/18 [==============================] - 15s 845ms/step - loss: 1.6298 - accuracy: 0.4768 - val_loss: 3.0203 - val_accuracy: 0.2249\n",
      "Epoch 29/80\n",
      "18/18 [==============================] - 15s 816ms/step - loss: 1.5117 - accuracy: 0.5133 - val_loss: 3.0435 - val_accuracy: 0.2462\n",
      "Epoch 30/80\n",
      "18/18 [==============================] - 17s 925ms/step - loss: 1.4363 - accuracy: 0.5308 - val_loss: 3.0567 - val_accuracy: 0.2219\n",
      "Epoch 31/80\n",
      "18/18 [==============================] - 17s 920ms/step - loss: 1.3363 - accuracy: 0.5657 - val_loss: 3.0834 - val_accuracy: 0.2280\n",
      "Epoch 32/80\n",
      "18/18 [==============================] - 15s 834ms/step - loss: 1.2572 - accuracy: 0.5897 - val_loss: 3.0917 - val_accuracy: 0.2310\n",
      "Epoch 33/80\n",
      "18/18 [==============================] - 15s 818ms/step - loss: 1.1891 - accuracy: 0.6052 - val_loss: 3.1226 - val_accuracy: 0.2462\n",
      "Epoch 34/80\n",
      "18/18 [==============================] - 14s 798ms/step - loss: 1.1509 - accuracy: 0.6246 - val_loss: 3.1147 - val_accuracy: 0.2416\n",
      "Epoch 35/80\n",
      "18/18 [==============================] - 15s 826ms/step - loss: 1.0621 - accuracy: 0.6520 - val_loss: 3.1701 - val_accuracy: 0.2416\n",
      "Epoch 36/80\n",
      "18/18 [==============================] - 15s 840ms/step - loss: 0.9948 - accuracy: 0.6782 - val_loss: 3.2198 - val_accuracy: 0.2477\n",
      "Epoch 37/80\n",
      "18/18 [==============================] - 15s 838ms/step - loss: 0.9779 - accuracy: 0.6759 - val_loss: 3.2087 - val_accuracy: 0.2584\n",
      "Epoch 38/80\n",
      "18/18 [==============================] - 15s 824ms/step - loss: 0.8964 - accuracy: 0.7055 - val_loss: 3.2781 - val_accuracy: 0.2675\n",
      "Epoch 39/80\n",
      "18/18 [==============================] - 14s 805ms/step - loss: 0.8352 - accuracy: 0.7409 - val_loss: 3.3434 - val_accuracy: 0.2614\n",
      "Epoch 40/80\n",
      "18/18 [==============================] - 15s 838ms/step - loss: 0.7617 - accuracy: 0.7618 - val_loss: 3.3198 - val_accuracy: 0.2705\n",
      "Epoch 41/80\n",
      "18/18 [==============================] - 15s 841ms/step - loss: 0.7049 - accuracy: 0.7823 - val_loss: 3.3931 - val_accuracy: 0.2796\n",
      "Epoch 42/80\n",
      "18/18 [==============================] - 15s 824ms/step - loss: 0.6805 - accuracy: 0.7831 - val_loss: 3.4130 - val_accuracy: 0.2675\n",
      "Epoch 43/80\n",
      "18/18 [==============================] - 16s 874ms/step - loss: 0.6441 - accuracy: 0.7998 - val_loss: 3.5106 - val_accuracy: 0.2903\n",
      "Epoch 44/80\n",
      "18/18 [==============================] - 15s 852ms/step - loss: 0.5925 - accuracy: 0.8142 - val_loss: 3.5657 - val_accuracy: 0.2812\n",
      "Epoch 45/80\n",
      "18/18 [==============================] - 15s 860ms/step - loss: 0.5782 - accuracy: 0.8309 - val_loss: 3.6092 - val_accuracy: 0.2736\n",
      "Epoch 46/80\n",
      "18/18 [==============================] - 15s 843ms/step - loss: 0.5224 - accuracy: 0.8416 - val_loss: 3.6037 - val_accuracy: 0.2903\n",
      "Epoch 47/80\n",
      "18/18 [==============================] - 15s 825ms/step - loss: 0.5006 - accuracy: 0.8480 - val_loss: 3.6500 - val_accuracy: 0.2903\n",
      "Epoch 48/80\n",
      "18/18 [==============================] - 15s 860ms/step - loss: 0.4759 - accuracy: 0.8533 - val_loss: 3.6886 - val_accuracy: 0.2994\n",
      "Epoch 49/80\n",
      "18/18 [==============================] - 15s 812ms/step - loss: 0.4532 - accuracy: 0.8689 - val_loss: 3.7121 - val_accuracy: 0.2948\n",
      "Epoch 50/80\n",
      "18/18 [==============================] - 14s 786ms/step - loss: 0.4147 - accuracy: 0.8754 - val_loss: 3.7794 - val_accuracy: 0.3009\n",
      "Epoch 51/80\n",
      "18/18 [==============================] - 14s 805ms/step - loss: 0.3916 - accuracy: 0.8830 - val_loss: 3.8017 - val_accuracy: 0.2933\n",
      "Epoch 52/80\n",
      "18/18 [==============================] - 14s 800ms/step - loss: 0.3646 - accuracy: 0.8993 - val_loss: 3.8929 - val_accuracy: 0.2979\n",
      "Epoch 53/80\n",
      "18/18 [==============================] - 14s 792ms/step - loss: 0.3532 - accuracy: 0.8997 - val_loss: 3.9306 - val_accuracy: 0.2796\n",
      "Epoch 54/80\n",
      "18/18 [==============================] - 14s 780ms/step - loss: 0.3348 - accuracy: 0.9050 - val_loss: 3.9426 - val_accuracy: 0.3100\n",
      "Epoch 55/80\n",
      "18/18 [==============================] - 14s 767ms/step - loss: 0.3212 - accuracy: 0.9069 - val_loss: 3.9844 - val_accuracy: 0.3024\n",
      "Epoch 56/80\n",
      "18/18 [==============================] - 14s 777ms/step - loss: 0.2856 - accuracy: 0.9236 - val_loss: 4.0204 - val_accuracy: 0.2964\n",
      "Epoch 57/80\n",
      "18/18 [==============================] - 14s 799ms/step - loss: 0.2699 - accuracy: 0.9233 - val_loss: 4.0556 - val_accuracy: 0.2842\n",
      "Epoch 58/80\n",
      "18/18 [==============================] - 14s 808ms/step - loss: 0.2684 - accuracy: 0.9225 - val_loss: 4.0407 - val_accuracy: 0.2948\n",
      "Epoch 59/80\n",
      "18/18 [==============================] - 14s 777ms/step - loss: 0.2487 - accuracy: 0.9358 - val_loss: 4.1088 - val_accuracy: 0.2979\n",
      "Epoch 60/80\n",
      "18/18 [==============================] - 14s 770ms/step - loss: 0.2527 - accuracy: 0.9335 - val_loss: 4.1124 - val_accuracy: 0.3100\n",
      "Epoch 61/80\n",
      "18/18 [==============================] - 14s 778ms/step - loss: 0.2339 - accuracy: 0.9324 - val_loss: 4.1436 - val_accuracy: 0.2979\n",
      "Epoch 62/80\n",
      "18/18 [==============================] - 15s 813ms/step - loss: 0.2158 - accuracy: 0.9422 - val_loss: 4.1838 - val_accuracy: 0.3009\n",
      "Epoch 63/80\n",
      "18/18 [==============================] - 15s 859ms/step - loss: 0.2030 - accuracy: 0.9453 - val_loss: 4.2138 - val_accuracy: 0.3040\n",
      "Epoch 64/80\n",
      "18/18 [==============================] - 15s 848ms/step - loss: 0.1970 - accuracy: 0.9506 - val_loss: 4.2478 - val_accuracy: 0.3191\n",
      "Epoch 65/80\n",
      "18/18 [==============================] - 14s 780ms/step - loss: 0.1977 - accuracy: 0.9445 - val_loss: 4.3372 - val_accuracy: 0.3085\n",
      "Epoch 66/80\n",
      "18/18 [==============================] - 15s 827ms/step - loss: 0.1862 - accuracy: 0.9514 - val_loss: 4.3606 - val_accuracy: 0.3055\n",
      "Epoch 67/80\n",
      "18/18 [==============================] - 15s 857ms/step - loss: 0.1846 - accuracy: 0.9536 - val_loss: 4.3735 - val_accuracy: 0.3040\n",
      "Epoch 68/80\n",
      "18/18 [==============================] - 14s 793ms/step - loss: 0.1740 - accuracy: 0.9521 - val_loss: 4.4224 - val_accuracy: 0.3009\n",
      "Epoch 69/80\n",
      "18/18 [==============================] - 18s 1s/step - loss: 0.1705 - accuracy: 0.9555 - val_loss: 4.4232 - val_accuracy: 0.3009\n",
      "Epoch 70/80\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.1624 - accuracy: 0.9605 - val_loss: 4.4800 - val_accuracy: 0.3009\n",
      "Epoch 71/80\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.1671 - accuracy: 0.9590 - val_loss: 4.5367 - val_accuracy: 0.2994\n",
      "Epoch 72/80\n",
      "18/18 [==============================] - 18s 1s/step - loss: 0.1409 - accuracy: 0.9643 - val_loss: 4.5366 - val_accuracy: 0.3176\n",
      "Epoch 73/80\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.1432 - accuracy: 0.9643 - val_loss: 4.5883 - val_accuracy: 0.3116\n",
      "Epoch 74/80\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.1427 - accuracy: 0.9574 - val_loss: 4.5809 - val_accuracy: 0.3116\n",
      "Epoch 75/80\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.1386 - accuracy: 0.9624 - val_loss: 4.5708 - val_accuracy: 0.3070\n",
      "Epoch 76/80\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.1311 - accuracy: 0.9658 - val_loss: 4.6423 - val_accuracy: 0.3116\n",
      "Epoch 77/80\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.1218 - accuracy: 0.9707 - val_loss: 4.6365 - val_accuracy: 0.3131\n",
      "Epoch 78/80\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.1272 - accuracy: 0.9647 - val_loss: 4.6874 - val_accuracy: 0.2994\n",
      "Epoch 79/80\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.1115 - accuracy: 0.9734 - val_loss: 4.7113 - val_accuracy: 0.3055\n",
      "Epoch 80/80\n",
      "18/18 [==============================] - 19s 1s/step - loss: 0.1115 - accuracy: 0.9719 - val_loss: 4.7215 - val_accuracy: 0.3055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2032f5eb670>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compile the model using Adam optimizer\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "#train the model on training sets and validate on testing sets\n",
    "model.fit(\n",
    "    x_train,y_train,\n",
    "    batch_size=150,epochs=80, \n",
    "    validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_12_layer_call_fn, lstm_cell_12_layer_call_and_return_conditional_losses, lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MOD\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MOD\\assets\n"
     ]
    }
   ],
   "source": [
    "#save the model for predictions\n",
    "model.save(\"MOD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n"
     ]
    }
   ],
   "source": [
    "#load the model\n",
    "model=load_model(\"MOD\")\n",
    "#generate random index\n",
    "index = np.random.randint(0,len(x_test)-1)\n",
    "#get the data of generated index from x_test\n",
    "music_pattern = x_test[index]\n",
    "\n",
    "out_pred=[] #it will store predicted notes\n",
    "\n",
    "#iterate till 200 note is generated\n",
    "for i in range(200):\n",
    "\n",
    "  #reshape the music pattern \n",
    "  music_pattern = music_pattern.reshape(1,len(music_pattern),1)\n",
    "  \n",
    "  #get the maximum probability value from the predicted output\n",
    "  pred_index = np.argmax(model.predict(music_pattern))\n",
    "  #get the note using predicted index and\n",
    "  #append to the output prediction list\n",
    "  out_pred.append(ind2note[pred_index])\n",
    "  music_pattern = np.append(music_pattern,pred_index)\n",
    "  \n",
    "  #update the music pattern with one timestep ahead\n",
    "  music_pattern = music_pattern[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI_composed_musicfinal.mid'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_notes = []\n",
    "for offset,pattern in enumerate(out_pred):\n",
    "  #if pattern is a chord instance\n",
    "  if ('.' in pattern) or pattern.isdigit():\n",
    "    #split notes from the chord\n",
    "    notes_in_chord = pattern.split('.')\n",
    "    notes = []\n",
    "    for current_note in notes_in_chord:\n",
    "        i_curr_note=int(current_note)\n",
    "        #cast the current note to Note object and\n",
    "        #append the current note \n",
    "        new_note = note.Note(i_curr_note)\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        notes.append(new_note)\n",
    "    \n",
    "    #cast the current note to Chord object\n",
    "    #offset will be 1 step ahead from the previous note\n",
    "    #as it will prevent notes to stack up \n",
    "    new_chord = chord.Chord(notes)\n",
    "    new_chord.offset = offset\n",
    "    output_notes.append(new_chord)\n",
    "  \n",
    "  else:\n",
    "    #cast the pattern to Note object apply the offset and \n",
    "    #append the note\n",
    "    new_note = note.Note(pattern)\n",
    "    new_note.offset = offset\n",
    "    new_note.storedInstrument = instrument.Piano()\n",
    "    output_notes.append(new_note)\n",
    "\n",
    "#save the midi file \n",
    "midi_stream = stream.Stream(output_notes)\n",
    "midi_stream.write('midi', fp='AI_composed_musicfinal.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f45b350507f7aaf4f6c587a8f796654f268faa79b6f528b46d4e262fdf664f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
